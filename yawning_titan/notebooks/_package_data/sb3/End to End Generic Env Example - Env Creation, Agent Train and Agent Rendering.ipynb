{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new `GenericNetworkEnv()` Open AI gym using YAWNING TITAN (YT)\n",
    "\n",
    "This notebook provides an end to end example of creating an environment and training a Proximal Policy Optimisation (PPO) agent within it.\n",
    "\n",
    "For the purposes of this example, we are going to first create an environment that has the same network topology as [Ridley 2017](https://www.nsa.gov.Portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-22-1.pdf#page=9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.helpers import network_creator\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.config.network_config.network_config import NetworkConfig\n",
    "from yawning_titan.config.game_modes import default_game_mode_path\n",
    "from yawning_titan.config.game_config.game_mode_config import GameModeConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the sceanrio's settings file\n",
    "\n",
    "Alongside a network, YT environments also need the scenario settings file. This includes a wide range of configurable parameters that shape how the sceanrio works such as the red agents goal, the blue agents observation space and much more. We'll just use the default one for this tutorial but please read the examples provides in `yawning_titan.config.game_modes` for a feel for the flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_mode_config = GameModeConfig.create_from_yaml(default_game_mode_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Network Representation\n",
    "\n",
    "YAWNING TITAN generic network environments rely on being given a network topology. YT has a number of in-built methods that are capable of generating networks but they can be user supplied. In the example below, we use the `network_creator.create_18_node_network()` to create the topology derived from Ridley 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, node_positions = network_creator.create_18_node_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create entry nodes (optional)\n",
    "\n",
    "You can also optionally supply red entry nodes. These are nodes that the red agent can attack from from timestep 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_nodes = [\"0\", \"1\", \"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of `NetworkConfig`\n",
    "\n",
    "The `NetworkConfig` class holds the network representation, entry nodes, high value nodes, and vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config = NetworkConfig.create_from_args(matrix=matrix, positions=node_positions, entry_nodes=entry_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Interface Object\n",
    "\n",
    "The network representation and the sceanrio configuration are then combined together to create a `NetworkInterface()` - This can be thought of as the red and blue agents primary point of interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_interface = NetworkInterface(game_mode=game_mode_config, network=network_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the red and blue agents\n",
    "\n",
    "Now that we have an `NetworkInterface()`, the next stage is to create Red and Blue interfaces to provide agents a means of interacting with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = RedInterface(network_interface)\n",
    "blue = BlueInterface(network_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment\n",
    "\n",
    "The `NetworkInterface()` can now be combined with the red and blue agent interfaces to create a `GenericNetworkEnv()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GenericNetworkEnv(red, blue, network_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the environment passes checks\n",
    "\n",
    "Once created, it's always worth checking the environment to see if its compliant with OpenAI Gym. For this, we can use the `check_env()` function provided by Stable Baselines 3. Silence means we are all good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env, warn=True)\n",
    "\n",
    "# reset anything changed during the check\n",
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(PPOMlp, env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(agent, env, n_eval_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
